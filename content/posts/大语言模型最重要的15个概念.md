---
title: "语言模型最重要的15个概念"
date: "2024-05-01"
tags: ["科普", "Python", "概念", "AI", "LLM"]
categories: ["概念"]
summary: "今天我们来给大家补课，我们在之前的教程中，带领大家搭建了本地的大语言模型服务。但是对于很多观众而言。大语言模型相关的一些技术概念仍然晦涩难懂，今天我们就一次性为大家解读清楚LLM最重要的15个概念。"
---

# 大语言模型最重要的15个概念

各位观众老爷大家好，今天我们来给大家补课，我们在之前的教程中，带领大家搭建了本地的大语言模型服务。但是对于很多观众而言。大语言模型相关的一些技术概念仍然晦涩难懂，今天我们就一次性为大家解读清楚。

## 神经网络（Neural Networks）：想象成一种模仿人脑工作方式的计算系统，通过大量数据学习如何执行任务

现在，我们所熟知的ai产品。都是构建在神经网络架构之上的计算机技术。这种技术既包括软件算法也包括硬件处理机制。

神经网络技术，最早可以追溯到1940年代，但是限于当时的机械式计算机的计算能力，这一算法的实际应用场景十分有限。1960年代和1980年代，神经网络技术出现过两次热潮。但最终被1995年出现的支持向量机（SVM）技术压过风头。
2006年开始神经网络技术迎来复苏。2010年往后，卷积神经网络(CNN)和递归神经网络(RNN)相继主导了几乎所有你能看到的人工智能产品，包括图像识别、语音识别、语义识别、自动翻译等等。

所谓神经网络，就是用计算机来模仿生物神经处理信息的过程，概要来说，就是让每个计算单元仅仅只处理非常简单的底层逻辑，通过这些简单的计算单元，层层堆叠来实现复杂的上层业务。

## Transformer模型：一种基于注意力机制的模型，广泛应用于处理序列数据

没错，Transformer的名称来自于著名的IP变形金刚。到我们说到transformer模型时，是指的transformer模型架构，或者基于Transformer架构的某一个模型。Transformer模型是一种基于注意力机制的神经网络架构，由Google在2017年的论文《Attention Is All You Need》中首次提出。它主要用于处理序列到序列（Seq2Seq）的任务，如机器翻译、文本摘要、问答系统等。Transformer模型的创新是完全放弃了之前广泛使用的循环神经网络（RNN）和卷积神经网络（CNN），转而采用全注意力机制来处理序列数据。这种设计使得模型可以更有效地并行处理数据，大大提高了训练速度和效率。 GPT、GPT2、ChatGPT是Transformer模型的典型代表。

## 生成式模型（Generative Models）：能够创建新内容（如文本）的模型

2017年，Transformer模型横空出世。几乎在所有的应用领域都击败了CNN和RNN，并且创造出了根据经验主动生成文字、图片、音频、视频的全新人工智能产品线。这项技术被称为生成式（Generative）。“生成式”这个词汇在英语当中也并非一个很常见的词汇。但是现在我们已经通过一系列产品，例如ChatGPT、Midjourney，理解了他的含义——也就是说，我们现在制造出了具有一定“创造力”的人工智能，我们可以通过这些人工智能生成那些从前不存在的信息。这一点与以前的AI技术形成了鲜明的对比。

## 参数（Parameters）：模型中的变量，通过训练过程调整以优化性能。

参数是我们用来表示一个模型规模大小的指标，常见的参数有70亿（也就是7B，B表示Billion）、130亿（13B)、340亿（34B）、700亿（70B）等。你可以将每一个参数理解为一个神经元，就像神经元一样参数只负责非常单一的任务。通常，具有更大规模参数的模型性能更强，就像拥有更多神经元的猴子，比鸟要更加聪明一样。

但是，具有更大的参数，意味着需要更多的计算能力。超过14b的模型很难在消费级的计算机上运行。

## 量化模型（Low-Bit Quantization）：将Transformer模型参数量化到8位、16位甚至更低位宽的表示

在机器学习和深度学习领域，量化模型通常指的是一种特定的技术或过程，旨在减少模型的大小和提高其运行速度。这包括将模型中的浮点参数转换为整数参数，以实现模型压缩。具体来说，模型量化涉及到将模型的参数从高精度（如FP32）映射到低精度（如4bit位，这种模型通常会标注q4_0），同时尽量减少精度损失。这一过程不仅有助于降低模型的计算需求和内存占用，还能够在保持较高预测精度的同时，加速模型的推理速度。

Transformer的模型可以使用双精度（FP64）、单精度（FP32）半精度（FP16）、8比特位（Int8）、4比特位（Int4）等不同精度的数据来存储每一个参数。注意哟，“比特位”并不是“位”，斯比特位并不是指的4位数的数，。压缩模型在理论上仍然是会造成性能损失的。

## 上下文（Context）：模型在生成或理解语言时考虑的周围单词或句子

生成式模型的特点是依赖上下文推理，猜测接下来将要出现的内容搭配。这种内容搭配可以是文字、图形（也就是颜色和位置）以及声音。就像我们学习语言时，会学习习惯用法和固定搭配。对于计算机而言，逻辑、情感、风格，甚至语言种类都只不过是一种习惯用法或固定搭配而已。模型通过对上下文的分析（对于中文使用者，这里通常指的是用户输入的“上文”，即关键字、提示词、指令），预测符合该上下文的输出内容。

## 上下文长度（Context Length）：模型在生成或理解文本时能够考虑的上下文信息的最大长度

Transformer模型在处理上下文长度上存在限制。在大语言模型面市的早期，这个限制通常是4096个Token（4k），这大约是3000个单词或者2000多个汉字。它既包含输入的内容，也包含了输出的内容。输入和输出相加，整体不能超过这个上限。现在有越来越多的大模型，将上限提到了8192个Token（8k）以上。这主要是由于Transformer模型依赖于注意力机制。当上下文长度增长时，计算量呈指数级增长，为了在工程上保持模型可用，当前Transformer模型设计时会设置一个固定的最大上下文长度。研究者们正通过探索新的架构和技术，努力提升模型处理更长上下文的能力，目前，已经有一些模型提供高达128k Token的上下文输入输出能力。在其他的生成式模型中，也存在类似的限制。例如stable diffusion限制提示词的长度和输出图片的尺寸。

## 大型语言模型（LLMs）：通过Transformer模型分析海量的文本数据来深入理解并运用人类语言的一类模型。

大语言模型是我们前面教程讨论的核心，它是transformer模型应用实践中最早的一个种类，主要用来处理文本数据。他通过模拟人的思考，执行各种语言相关的任务，如回答问题、撰写文本、翻译不同语言以及模仿特定写作风格等。它们具备了从大量阅读材料中学习的能力，从而能够生成文本、理解复杂语境，并提供信息丰富的回应。现在，大型语言模型已经广泛应用于教育、新闻、娱乐、客户服务和科研等多个领域。

## 预训练（Pre-training）：在大量数据上训练模型以学习通用的输出

构造一个Transformer模型分为三步：首先是设计模型架构，包括参数规模、网络层数等；其次，是进行预训练，填充初始知识；最后，是进行特定的微调。预训练过程是一个非常消耗计算资源的过程，LLaMa3的预训练耗费了约2.4万张H100计算卡数月时间（每张卡价值20万）。如果你想预训练一个最简单的大语言模型，至少要给它提供完整的语言教材和阅读材料，让它学习语言的基本知识，比如单词的意思、句子的结构等，这样模型才能获得一定的语言理解能力。

预训练是很有可能失败的。如果模型架构设计不合理，或者输入的数据相关度不高，无法形成收敛，最终导致模型无法拟合。一旦预训练失败，就会浪费掉了大量的计算资源。

## 微调（Fine-tuning）：在预训练模型的基础上，针对特定任务进行进一步训练，以提高性能

在模型通过预训练掌握了基础知识后，我们通常还需要对它进行“微调”。有时候，微调就像是给它一个特定的任务，比如翻译或回答问题，并让它在少量的相关数据上进行学习，以便更好地完成这个任务。另外一些时候，微调的目的是出于安全考量。我们通过微调给模型设置安全边界，让它拒绝回答有毒的问题，或者输出危害社会的结果，一个典型的例子是，防止它编写计算机病毒。微调有多种办法，现在最流行的微调方式是微软发布的LoRa方法。这种方法可以在模型之外加载独立的微调模型，提升模型的能力。这些微调模型还可以被合并到主模型中，使其成为预训练的一部分。

## 提示词（prompt）（或引导语）：给大语言模型开始推理工作的起点

在大语言模型中，prompt通常是一段文本，可以是一个问题，比如，“请解释什么是光合作用？”，或者一个请求，比如，“给我写一封正式的商务邮件。”在所有这些情况中，prompt的作用都是给模型一个开始工作的方向或者起点。模型会根据prompt的内容，利用它在预训练阶段学到的知识，来生成回应或者完成任务。

为了让大语言模型更好的完成我们指定的任务，我们还会设置”系统提示词“（system prompt）。系统提示词会帮助大语言模型识别当前请求的更多背景，以提供更好的输出内容。比如说当我问他请解释什么是光合作用时，我可以设置一个系统提示词：“你是一位植物园的讲解员”。这样将会帮助大语言模型输出更好的答案。

这个听起来有点不可思议，为什么我给大语言模型设置了特别的人设之后，他就能更好地回答我的问题呢？这是因为这些信息都已经通过预训练存储在了大模型内。你必须要设置相应的提示词，才能够检索到这一部分信息。这就好像你记住了某个故事，但是只有在特定的场景才能够激起你的回忆。

## 知识库（Knowledge Base）：一组用来帮助模型服务于特定领域的结构化的信息

在人工智能系统中，知识库用来存储重要的事实和关系。在实践中，知识库可以是一组Word文档，或者图片，或者表格，或者代码片段。当系统需要回答复杂的问题或执行需要背景知识的任务时，我们就会为系统创建相关的知识库。简而言之，知识库就像是AI系统的“辅助记忆”或者“课堂笔记”，帮助它利用额外信息更好地理解和回答问题。

## **RAG（Retrieval-Augmented Generation）**： 想象你是一位作家，正在尝试写一篇关于特定主题的文章。

你需要写一篇关于德古拉吸血鬼的报告，即便你非常喜欢这个话题，但你所知道的信息仍然有限，因此需要依赖许多与这个主题相关的书籍和资料，才能完成写作。RAG方法也是也是类似的，它结合了两个过程：首先，它像图书馆检索一样，从知识库中检索相关信息；然后，它像作家一样，利用检索到的信息生成文本。

这是一种非常好的绕过Transformer有限的上下文限制的方法。具体过程包括：1）知识库的向量化；2）根据需求检索向量数据；3）模型通过向量数据补充的信息进行推理；4）输出包含知识库信息的答案。

实际上，New Bing或者Metaso等AI搜索引擎就是采用知识库的方法回答我们的提问，只不过他们采用的知识库就是搜索引擎中的一个个网页。

## 嵌入（Embeddings）：将词汇或短语转换成高维空间中的向量，以捕捉其含义

嵌入是将单词或短语转换成数字形式（向量）的方法，这样计算机可以理解和处理它们。通常我们在进行RAG任务的时候，会对我们的嗯知识库进行嵌入处理。嵌入后的文本呈现为一个数组，它们大概长成这样：[43.12,5.18,74.9,6.55.....]。你可以在向量库管理软件中看到这些数据，但是对于你而言，这些数据并没有任何明确的含义。你可以把这个过程想象成给每个单词一个独特的指纹，这个指纹捕捉了单词的某些特性，使得计算机可以通过这些指纹来识别和比较单词。

## 零次学习（Zero-Shot Learning）：模型在没有明确训练的情况下，能够处理未见过的类别或任务。

这是指在没有任何直接指导的情况下，尝试完成某项任务。就像是一个经验丰富的厨师，即使没有被特别教过如何做一道新菜，也能够根据他对烹饪的理解和经验来尝试制作。在语言模型中，零次学习意味着模型尝试去理解和执行它没有直接训练过的任务，这通常依赖于它在预训练阶段获得的广泛知识。有时候，我们也将零次学习的能力描述为“涌现”，也就是说模型产生了我们没有预期到的能力。

参考资料

1.  [大模型理论基础](https://datawhalechina.github.io/so-large-lm/)
2.  [大规模语言模型：从理论到实践](https://intro-llm.github.io/)
3. [AI大型语言模型的基础理论与实践原创](https://blog.csdn.net/m0_62554628/article/details/136773270)
4. [终于有人把自然语言处理、机器学习、深度学习和ai讲明白了 - 知乎](https://zhuanlan.zhihu.com/p/159223343)
5. [大语言模型定义、概念介绍](https://developer.aliyun.com/article/1474409)
6. [掌握大型语言模型指南 - Unite.AI](https://unite.ai/zh-CN/%E6%8E%8C%E6%8F%A1%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8C%87%E5%8D%97/) [2024-01-24]
7. [大语言模型综述 - Renmin University of China](http://ai.ruc.edu.cn/research/science/20230605100.html) [2023-06-05]
8. [大语言模型基本概念（学习笔记） 转载](https://blog.csdn.net/baidu_39417409/article/details/136170092)
9. [全面解析大语言模型的工作原理 - 知乎 - 知乎专栏](https://zhuanlan.zhihu.com/p/647511022)